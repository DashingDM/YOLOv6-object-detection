# -*- coding: utf-8 -*-
"""Lab_3_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aVARgILsGaxPVxQdbFEJFT4o5sE-XZic
"""

LAST_NAME = "Mehta"
FIRST_NAME = "Dharm"

# Training parameters
EPOCHS = 50
BATCH_SIZE = 32

!nvidia-smi

import os
import subprocess
import sys
from pathlib import Path
import shutil
import random
from google.colab import drive

# Check GPU availability
print("\n Checking GPU...")
gpu_info = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
if gpu_info.returncode == 0:
    print("GPU detected!")
    print(gpu_info.stdout)
else:
    print(" No GPU detected. Training will be very slow on CPU.")

# Mount Google Drive for saving results
print("\n" + "="*70)
print("Mounting Google Drive...")
print("="*70)
try:
    drive.mount('/content/drive')
    SAVE_TO_DRIVE = True
    DRIVE_PATH = '/content/drive/MyDrive/EECE7398_Lab3'
    os.makedirs(DRIVE_PATH, exist_ok=True)
    print(f"Drive mounted! Results will save to: {DRIVE_PATH}")
except Exception as e:
    SAVE_TO_DRIVE = False
    print("  Drive not mounted. Results will only be in Colab session.")
    print("   (Click 'Cancel' if you don't want to mount)")

print("="*70)

# Clone YOLOv6 repository
if not os.path.exists("YOLOv6"):
    print("Cloning YOLOv6 repository...")
    !git clone https://github.com/meituan/YOLOv6.git
    print("YOLOv6 cloned successfully")
else:
    print("YOLOv6 already exists")

# Change to YOLOv6 directory
os.chdir("/content/YOLOv6")
print(f"Working directory: {os.getcwd()}")

# Install requirements
print("\nInstalling Python packages...")

!pip uninstall -y onnx-simplifier onnxsim
!pip install onnx==1.16.1 onnxsim==0.4.33 addict==2.4.0
!grep -v "onnx-simplifier" requirements.txt > fixed_requirements.txt
!pip install -r fixed_requirements.txt
!pip install gdown

!pip install -q wandb tensorboard pillow tqdm

print(" All dependencies installed!")

os.makedirs("weights", exist_ok=True)

if not os.path.exists("weights/yolov6n.pt"):
    print("Downloading YOLOv6-n weights...")
    !wget -q --show-progress -P weights/ https://github.com/meituan/YOLOv6/releases/download/0.4.0/yolov6n.pt
    print("Model downloaded")
else:
    print("Model already exists")

os.makedirs("data", exist_ok=True)

# Download images
if not os.path.exists("data/images.tar.gz"):
    print("Downloading images (~750 MB)...")
    !wget -q --show-progress -P data/ https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz
else:
    print(" Images already downloaded")

# Download annotations
if not os.path.exists("data/annotations.tar.gz"):
    print("Downloading annotations...")
    !wget -q --show-progress -P data/ https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz
else:
    print(" Annotations already downloaded")

# Extract
print("Extracting...")
!tar -xzf data/images.tar.gz -C data/ 2>/dev/null
!tar -xzf data/annotations.tar.gz -C data/ 2>/dev/null
print(" Dataset ready")

print("\n" + "="*70)
print(" STEP 4: Converting Dataset to YOLO Format")
print("="*70)

import xml.etree.ElementTree as ET
from PIL import Image
from tqdm import tqdm
import random
import shutil

class PetsDatasetConverter:
    """Convert Oxford-IIIT Pet Dataset to YOLO format"""

    def __init__(self):
        self.data_dir = Path("data")
        self.images_dir = self.data_dir / "images"
        self.annotations_dir = self.data_dir / "annotations" / "xmls"
        self.output_dir = self.data_dir / "pets"

    def convert_bbox(self, size, box):
        """Convert bbox to YOLO format (normalized)"""
        dw = 1.0 / size[0]
        dh = 1.0 / size[1]
        x = (box[0] + box[1]) / 2.0
        y = (box[2] + box[3]) / 2.0
        w = box[1] - box[0]
        h = box[3] - box[2]
        x = x * dw
        w = w * dw
        y = y * dh
        h = h * dh
        return (x, y, w, h)

    def parse_xml(self, xml_file):
        """Parse XML annotation file"""
        tree = ET.parse(xml_file)
        root = tree.getroot()

        size = root.find('size')
        width = int(size.find('width').text)
        height = int(size.find('height').text)

        objects = []
        for obj in root.findall('object'):
            name = obj.find('name').text
            bndbox = obj.find('bndbox')
            xmin = float(bndbox.find('xmin').text)
            ymin = float(bndbox.find('ymin').text)
            xmax = float(bndbox.find('xmax').text)
            ymax = float(bndbox.find('ymax').text)

            objects.append({
                'name': name,
                'bbox': (xmin, xmax, ymin, ymax),
                'size': (width, height)
            })

        return objects

    def create_class_mapping(self):
        """Create class name to ID mapping"""
        classes = set()
        for xml_file in self.annotations_dir.glob("*.xml"):
            tree = ET.parse(xml_file)
            root = tree.getroot()
            for obj in root.findall('object'):
                name = obj.find('name').text
                classes.add(name)

        classes = sorted(list(classes))
        class_to_id = {name: idx for idx, name in enumerate(classes)}

        # Save class names
        with open(self.output_dir / "classes.txt", "w") as f:
            for name in classes:
                f.write(f"{name}\n")

        return class_to_id, classes

    def convert(self, train_split=0.7, val_split=0.2):
        """Convert dataset to YOLO format"""
        print("Converting Oxford-IIIT Pet Dataset to YOLO format...")

        # Create output directories
        for split in ['train', 'val', 'test']:
            (self.output_dir / 'images' / split).mkdir(parents=True, exist_ok=True)
            (self.output_dir / 'labels' / split).mkdir(parents=True, exist_ok=True)

        # Create class mapping
        class_to_id, classes = self.create_class_mapping()
        print(f" Found {len(class_to_id)} pet classes")

        # Get all annotation files
        xml_files = list(self.annotations_dir.glob("*.xml"))
        random.shuffle(xml_files)

        # Split dataset
        n = len(xml_files)
        train_n = int(n * train_split)
        val_n = int(n * val_split)

        splits = {
            'train': xml_files[:train_n],
            'val': xml_files[train_n:train_n + val_n],
            'test': xml_files[train_n + val_n:]
        }

        # Convert each split
        for split_name, files in splits.items():
            print(f"\nðŸ“‚ Processing {split_name} split ({len(files)} images)...")

            converted = 0
            for xml_file in tqdm(files, desc=f"Converting {split_name}"):
                # Parse XML
                try:
                    objects = self.parse_xml(xml_file)
                    if not objects:
                        continue
                except Exception as e:
                    continue

                # Find corresponding image
                img_name = xml_file.stem
                img_path = None
                for ext in ['.jpg', '.png', '.jpeg']:
                    potential_path = self.images_dir / f"{img_name}{ext}"
                    if potential_path.exists():
                        img_path = potential_path
                        break

                if not img_path or not img_path.exists():
                    continue

                # Copy image
                dst_img = self.output_dir / 'images' / split_name / img_path.name
                shutil.copy(img_path, dst_img)

                # Create label file
                label_path = self.output_dir / 'labels' / split_name / f"{img_name}.txt"
                with open(label_path, 'w') as f:
                    for obj in objects:
                        class_id = class_to_id[obj['name']]
                        bbox = self.convert_bbox(obj['size'], obj['bbox'])
                        f.write(f"{class_id} {' '.join(map(str, bbox))}\n")

                converted += 1

            print(f"    Converted {converted} images")

        print(f"\n{'='*70}")
        print(" CONVERSION COMPLETE!")
        print(f"{'='*70}")
        print(f"   Train: {len(splits['train'])} images")
        print(f"   Val:   {len(splits['val'])} images")
        print(f"   Test:  {len(splits['test'])} images")
        print(f"   Total Classes: {len(classes)}")
        print(f"{'='*70}")

        return len(classes), classes

# Run the conversion
print("Starting conversion...")
converter = PetsDatasetConverter()
num_classes, class_names = converter.convert()

print(f"\n Dataset ready for training!")
print(f"   Classes: {', '.join(class_names[:5])}... ({num_classes} total)")

print("\n" + "="*70)
print(" Creating Dataset Configuration")
print("="*70)

# First, let's check what we have
print(" Checking dataset structure...")
import os
from pathlib import Path

data_base = Path("/content/YOLOv6/data/pets")

for split in ['train', 'val', 'test']:
    img_dir = data_base / 'images' / split
    lbl_dir = data_base / 'labels' / split

    if img_dir.exists():
        img_count = len(list(img_dir.glob("*.*")))
        print(f"    {split}/images: {img_count} files")
    else:
        print(f"    {split}/images: NOT FOUND")

    if lbl_dir.exists():
        lbl_count = len(list(lbl_dir.glob("*.txt")))
        print(f"    {split}/labels: {lbl_count} files")
    else:
        print(f"    {split}/labels: NOT FOUND")

# Define Oxford-IIIT Pet Dataset classes (37 breeds)
classes = [
    'Abyssinian', 'Bengal', 'Birman', 'Bombay', 'British_Shorthair',
    'Egyptian_Mau', 'Maine_Coon', 'Persian', 'Ragdoll', 'Russian_Blue',
    'Siamese', 'Sphynx', 'american_bulldog', 'american_pit_bull_terrier',
    'basset_hound', 'beagle', 'boxer', 'chihuahua', 'english_cocker_spaniel',
    'english_setter', 'german_shorthaired', 'great_pyrenees', 'havanese',
    'japanese_chin', 'keeshond', 'leonberger', 'miniature_pinscher',
    'newfoundland', 'pomeranian', 'pug', 'saint_bernard', 'samoyed',
    'scottish_terrier', 'shiba_inu', 'staffordshire_bull_terrier',
    'wheaten_terrier', 'yorkshire_terrier'
]

num_classes = len(classes)

# Create YAML with absolute paths
yaml_content = f"""# Oxford-IIIT Pet Dataset Configuration
train: /content/YOLOv6/data/pets/images/train
val: /content/YOLOv6/data/pets/images/val
test: /content/YOLOv6/data/pets/images/test

nc: {num_classes}
names: {classes}
"""

yaml_path = "/content/YOLOv6/data/pets.yaml"
with open(yaml_path, "w") as f:
    f.write(yaml_content)

print(f"\n Created: {yaml_path}")
print(f"   Classes: {num_classes}")

# Verify the YAML content
print("\nðŸ“„ YAML content:")
with open(yaml_path, "r") as f:
    print(f.read())

# Double-check paths in YAML actually exist
print("\n Verifying YAML paths:")
import yaml

with open(yaml_path, "r") as f:
    config = yaml.safe_load(f)

for key in ['train', 'val', 'test']:
    if key in config:
        path = config[key]
        exists = os.path.exists(path)
        count = len(list(Path(path).glob("*.*"))) if exists else 0
        status = "" if exists else ""
        print(f"   {status} {key}: {path} ({count} files)")

print("\n Dataset configuration complete!")

!pip uninstall -y numpy

!pip install "numpy<2" --quiet

import os
from pathlib import Path

os.chdir("/content/YOLOv6")

# Recreate the YAML file
yaml_content = """# Oxford-IIIT Pet Dataset Configuration
train: /content/YOLOv6/data/pets/images/train
val: /content/YOLOv6/data/pets/images/val
test: /content/YOLOv6/data/pets/images/test

nc: 2
names: ['cat', 'dog']
"""

os.makedirs("data", exist_ok=True)

with open("data/pets.yaml", "w") as f:
    f.write(yaml_content)

print(" YAML recreated!")

# Verify it exists
print("\nðŸ” Verifying:")
print(f"   YAML exists: {os.path.exists('data/pets.yaml')}")

# Check if dataset directories exist
for split in ['train', 'val', 'test']:
    path = f"/content/YOLOv6/data/pets/images/{split}"
    exists = os.path.exists(path)
    count = len(list(Path(path).glob("*.*"))) if exists else 0
    print(f"   {split}: {exists} ({count} files)")

print("\n Now re-run the evaluation command!")

print("\n" + "="*70)
print(" EVALUATING PRETRAINED MODEL (BEFORE FINE-TUNING)")
print("="*70)

import os
from pathlib import Path
from tqdm import tqdm
import torch
import yaml
import numpy as np
import cv2

# Load config
with open('/content/YOLOv6/data/pets.yaml', 'r') as f:
    data_config = yaml.safe_load(f)

print(f"\n Dataset: {data_config['nc']} classes")

# Load model
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print(f" Loading model on {device}...")

ckpt = torch.load('weights/yolov6n.pt', map_location=device, weights_only=False)
model = ckpt['model'].float().to(device).eval()

params = sum(p.numel() for p in model.parameters()) / 1e6
print(f" Loaded: {params:.2f}M parameters")

# Get images
val_dir = Path("/content/YOLOv6/data/pets/images/val")
val_images = sorted(val_dir.glob('*.jpg'))[:100]

print(f"\n Processing {len(val_images)} images...")

# Inference
successful = 0
times = []

with torch.no_grad():
    for img_path in tqdm(val_images, desc="Inference"):
        try:
            img = cv2.imread(str(img_path))
            if img is None:
                continue

            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            img = cv2.resize(img, (640, 640))
            img = img.astype('float32') / 255.0
            img = np.transpose(img, (2, 0, 1))
            img_tensor = torch.from_numpy(img).unsqueeze(0).to(device)

            if device.type == 'cuda':
                start = torch.cuda.Event(enable_timing=True)
                end = torch.cuda.Event(enable_timing=True)
                start.record()
                output = model(img_tensor)
                end.record()
                torch.cuda.synchronize()
                times.append(start.elapsed_time(end))
            else:
                import time
                t0 = time.time()
                output = model(img_tensor)
                times.append((time.time() - t0) * 1000)

            successful += 1
        except:
            continue

avg_time = np.mean(times)
fps = 1000.0 / avg_time

print("\n" + "="*70)
print(" PRETRAINED MODEL RESULTS (BEFORE FINE-TUNING)")
print("="*70)
print(f"\n Statistics:")
print(f"   Images processed: {successful}/{len(val_images)}")
print(f"\nâš¡ Speed:")
print(f"   Inference: {avg_time:.2f} ms/image")
print(f"   FPS: {fps:.1f}")
print(f"\n Model:")
print(f"   YOLOv6-n: {params:.2f}M params")
print(f"   Pretrained: COCO (80 classes)")
print(f"   Target: Oxford Pets ({data_config['nc']} classes)")
print("\n" + "="*70)
print(" SCREENSHOT #1 - BASELINE BEFORE FINE-TUNING")
print("="*70)

# Save log
os.makedirs('logs', exist_ok=True)
with open('logs/pretrained_test.log', 'w') as f:
    f.write(f"PRETRAINED MODEL (BEFORE FINE-TUNING)\n")
    f.write(f"Images: {successful}\n")
    f.write(f"Inference: {avg_time:.2f} ms\n")
    f.write(f"FPS: {fps:.1f}\n")

print("\n Log saved!")

print("\n" + "="*70)
print(" EVALUATING PRETRAINED MODEL (BEFORE FINE-TUNING)")
print("="*70)

import torch
import yaml
import numpy as np
import cv2

# Load config
with open('/content/YOLOv6/data/pets.yaml', 'r') as f:
    data_config = yaml.safe_load(f)

print(f"\n Dataset: {data_config['nc']} classes")

# Load model
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print(f" Loading model on {device}...")

ckpt = torch.load('weights/yolov6n.pt', map_location=device, weights_only=False)
model = ckpt['model'].float().to(device).eval()

params = sum(p.numel() for p in model.parameters()) / 1e6
print(f" Loaded: {params:.2f}M parameters")

# Get images
val_dir = Path("/content/YOLOv6/data/pets/images/val")
val_images = sorted(val_dir.glob('*.jpg'))[:100]

print(f"\nProcessing {len(val_images)} images...")

# Inference
successful = 0
times = []

with torch.no_grad():
    for img_path in tqdm(val_images, desc="Inference"):
        try:
            img = cv2.imread(str(img_path))
            if img is None:
                continue

            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            img = cv2.resize(img, (640, 640))
            img = img.astype('float32') / 255.0
            img = np.transpose(img, (2, 0, 1))
            img_tensor = torch.from_numpy(img).unsqueeze(0).to(device)

            if device.type == 'cuda':
                start = torch.cuda.Event(enable_timing=True)
                end = torch.cuda.Event(enable_timing=True)
                start.record()
                output = model(img_tensor)
                end.record()
                torch.cuda.synchronize()
                times.append(start.elapsed_time(end))
            else:
                import time
                t0 = time.time()
                output = model(img_tensor)
                times.append((time.time() - t0) * 1000)

            successful += 1
        except:
            continue

avg_time = np.mean(times)
fps = 1000.0 / avg_time

print("\n" + "="*70)
print("ðŸ“Š PRETRAINED MODEL RESULTS (BEFORE FINE-TUNING)")
print("="*70)
print(f"\nðŸ“ˆ Statistics:")
print(f"   Images processed: {successful}/{len(val_images)}")
print(f"\nâš¡ Speed:")
print(f"   Inference: {avg_time:.2f} ms/image")
print(f"   FPS: {fps:.1f}")
print(f"\nðŸ¤– Model:")
print(f"   YOLOv6-n: {params:.2f}M params")
print(f"   Pretrained: COCO (80 classes)")
print(f"   Target: Oxford Pets ({data_config['nc']} classes)")
print("\n" + "="*70)
print(" SCREENSHOT #1 - BASELINE BEFORE FINE-TUNING")
print("="*70)

# Save log
os.makedirs('logs', exist_ok=True)
with open('logs/pretrained_test.log', 'w') as f:
    f.write(f"PRETRAINED MODEL (BEFORE FINE-TUNING)\n")
    f.write(f"Images: {successful}\n")
    f.write(f"Inference: {avg_time:.2f} ms\n")
    f.write(f"FPS: {fps:.1f}\n")

print("\n Log saved!")

print("TESTING PRETRAINED MODEL")


import torch
import yaml
import numpy as np
import cv2
import warnings
warnings.filterwarnings('ignore')

# Load config
with open('/content/YOLOv6/data/pets.yaml', 'r') as f:
    data_config = yaml.safe_load(f)

print(f"\n Dataset: {data_config['nc']} classes")

# Load model
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
ckpt = torch.load('weights/yolov6n.pt', map_location=device, weights_only=False)
model = ckpt['model'].float().to(device).eval()

params = sum(p.numel() for p in model.parameters()) / 1e6
print(f"Loaded: {params:.2f}M parameters")

# Get test data
test_dir = Path("/content/YOLOv6/data/pets/images/val")
labels_dir = Path("/content/YOLOv6/data/pets/labels/val")
test_images = sorted(test_dir.glob('*.jpg'))
print(f"\nTesting on {len(test_images)} images")

def process_predictions(output, conf_thresh=0.25, iou_thresh=0.01):  # INCREASED THRESHOLD
    """Process YOLOv6 output with proper confidence filtering"""
    if isinstance(output, (list, tuple)):
        output = output[0]

    if len(output.shape) == 3:
        output = output[0]  # [8400, 85]

    # Extract components
    boxes = output[:, :4]  # [x, y, w, h] in pixels
    objectness = output[:, 4]
    class_scores = output[:, 5:]  # 80 classes

    # Get class predictions
    class_conf, class_pred = class_scores.max(1)

    # Combined confidence
    conf = objectness * class_conf

    # CRITICAL: Filter by confidence EARLY
    mask = conf > conf_thresh

    print(f"  Filtered from {len(conf)} to {mask.sum().item()} predictions", end='\r')

    if mask.sum() == 0:
        return torch.tensor([])

    boxes = boxes[mask]
    conf = conf[mask]
    class_pred = class_pred[mask]

    # Normalize to 0-1
    boxes = boxes / 640.0

    # Simple NMS
    keep = []
    sorted_idx = conf.argsort(descending=True)

    while len(sorted_idx) > 0 and len(keep) < 100:  # Max 100 detections per image
        i = sorted_idx[0]
        keep.append(i.item())

        if len(sorted_idx) == 1:
            break

        # IoU with remaining
        box = boxes[i]
        other = boxes[sorted_idx[1:]]

        b1_x1 = box[0] - box[2]/2
        b1_y1 = box[1] - box[3]/2
        b1_x2 = box[0] + box[2]/2
        b1_y2 = box[1] + box[3]/2

        b2_x1 = other[:, 0] - other[:, 2]/2
        b2_y1 = other[:, 1] - other[:, 3]/2
        b2_x2 = other[:, 0] + other[:, 2]/2
        b2_y2 = other[:, 1] + other[:, 3]/2

        inter_x1 = torch.maximum(torch.tensor(b1_x1), b2_x1)
        inter_y1 = torch.maximum(torch.tensor(b1_y1), b2_y1)
        inter_x2 = torch.minimum(torch.tensor(b1_x2), b2_x2)
        inter_y2 = torch.minimum(torch.tensor(b1_y2), b2_y2)

        inter = torch.clamp(inter_x2 - inter_x1, min=0) * torch.clamp(inter_y2 - inter_y1, min=0)
        area1 = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)
        area2 = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)
        union = area1 + area2 - inter

        iou = inter / (union + 1e-6)

        keep_mask = iou < iou_thresh
        sorted_idx = sorted_idx[1:][keep_mask]

    if len(keep) == 0:
        return torch.tensor([])

    return torch.cat([
        boxes[keep],
        conf[keep].unsqueeze(1),
        class_pred[keep].unsqueeze(1).float()
    ], dim=1)

def box_iou(box1, box2):
    """IoU for normalized boxes"""
    b1_x1, b1_y1 = box1[0] - box1[2]/2, box1[1] - box1[3]/2
    b1_x2, b1_y2 = box1[0] + box1[2]/2, box1[1] + box1[3]/2
    b2_x1, b2_y1 = box2[0] - box2[2]/2, box2[1] - box2[3]/2
    b2_x2, b2_y2 = box2[0] + box2[2]/2, box2[1] + box2[3]/2

    inter_x1 = max(b1_x1, b2_x1)
    inter_y1 = max(b1_y1, b2_y1)
    inter_x2 = min(b1_x2, b2_x2)
    inter_y2 = min(b1_y2, b2_y2)

    inter = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)
    area1 = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)
    area2 = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)
    union = area1 + area2 - inter

    return inter / union if union > 0 else 0

# Run evaluation
print("\n  Running inference...")

all_preds = []
all_targets = []
times = []

# Open log file for real-time writing
log_file = open('logs/pretrained_test.log', 'w')

with torch.no_grad():
    for idx, img_path in enumerate(tqdm(test_images, desc="Testing", file=sys.stdout)):
        try:
            # Log progress every 100 images
            if idx % 100 == 0:
                progress_msg = f"Processed {idx}/{len(test_images)} images ({idx/len(test_images)*100:.1f}%)\n"
                log_file.write(progress_msg)
                log_file.flush()

            img = cv2.imread(str(img_path))
            if img is None:
                continue

            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            img = cv2.resize(img, (640, 640))
            img = img.astype('float32') / 255.0
            img = np.transpose(img, (2, 0, 1))
            img_tensor = torch.from_numpy(img).unsqueeze(0).to(device)

            if device.type == 'cuda':
                start = torch.cuda.Event(enable_timing=True)
                end = torch.cuda.Event(enable_timing=True)
                start.record()
                output = model(img_tensor)
                end.record()
                torch.cuda.synchronize()
                times.append(start.elapsed_time(end))

            preds = process_predictions(output, conf_thresh=0.25)

            label_file = labels_dir / f"{img_path.stem}.txt"
            targets = []
            if label_file.exists():
                with open(label_file, 'r') as f:
                    for line in f:
                        parts = line.strip().split()
                        if len(parts) >= 5:
                            cls, x, y, w, h = map(float, parts[:5])
                            targets.append([cls, x, y, w, h])

            all_preds.append(preds.cpu() if len(preds) > 0 else torch.tensor([]))
            all_targets.append(torch.tensor(targets) if targets else torch.tensor([]))

        except Exception as e:
            log_file.write(f"Error on image {idx}: {str(e)}\n")
            all_preds.append(torch.tensor([]))
            all_targets.append(torch.tensor([]))

log_file.write(f"Completed: {len(test_images)}/{len(test_images)} images (100.0%)\n")
log_file.close()
print()  # New line after progress

# Calculate mAP
def calculate_ap(predictions, targets, iou_thresh=0.5):
    """Calculate AP at given IoU threshold"""
    tp_list = []
    conf_list = []
    total_gt = sum(len(t) for t in targets)

    if total_gt == 0:
        return 0.0, 0.0, 0.0

    for pred, tgt in zip(predictions, targets):
        if len(pred) == 0:
            continue

        matched = set()
        for i in range(len(pred)):
            pred_box = pred[i, :4].numpy()
            pred_conf = pred[i, 4].item()

            best_iou = 0
            best_j = -1

            if len(tgt) > 0:
                for j in range(len(tgt)):
                    if j in matched:
                        continue
                    tgt_box = tgt[j, 1:5].numpy()
                    iou = box_iou(pred_box, tgt_box)
                    if iou > best_iou:
                        best_iou = iou
                        best_j = j

            if best_iou >= iou_thresh and best_j != -1:
                tp_list.append(1)
                matched.add(best_j)
            else:
                tp_list.append(0)

            conf_list.append(pred_conf)

    if len(tp_list) == 0:
        return 0.0, 0.0, 0.0

    indices = np.argsort(conf_list)[::-1]
    tp = np.array(tp_list)[indices]

    tp_cumsum = np.cumsum(tp)
    fp_cumsum = np.cumsum(1 - tp)

    recalls = tp_cumsum / total_gt
    precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-10)

    # AP = area under PR curve
    ap = 0
    for i in range(len(recalls) - 1):
        ap += (recalls[i+1] - recalls[i]) * precisions[i+1]

    return ap, precisions[-1], recalls[-1]

# Calculate metrics
map_50, prec_50, rec_50 = calculate_ap(all_preds, all_targets, 0.5)
map_75, _, _ = calculate_ap(all_preds, all_targets, 0.75)

aps = [calculate_ap(all_preds, all_targets, iou)[0] for iou in np.arange(0.5, 1.0, 0.05)]
map_50_95 = np.mean(aps)

total_preds = sum(len(p) for p in all_preds)
total_tgts = sum(len(t) for t in all_targets)
avg_time = np.mean(times)
fps = 1000.0 / avg_time


print("PRETRAINED MODEL TEST RESULTS (BEFORE FINE-TUNING)")


print(f"\n Accuracy Metrics:")
print(f"   mAP@0.5:0.95: {map_50_95:.3f}  ({map_50_95*100:.1f}%)")
print(f"   mAP@0.5:      {map_50:.3f}  ({map_50*100:.1f}%)")
print(f"   mAP@0.75:     {map_75:.3f}  ({map_75*100:.1f}%)")
print(f"   Precision:    {prec_50:.3f}  ({prec_50*100:.1f}%)")
print(f"   Recall:       {rec_50:.3f}  ({rec_50*100:.1f}%)")

print(f"   Statistics:")
print(f"   Images: {len(test_images)}")
print(f"   Predictions: {total_preds}")
print(f"   Ground truths: {total_tgts}")
print(f"   Avg preds/image: {total_preds/len(test_images):.1f}")

print(f"   Speed:")
print(f"   Inference: {avg_time:.2f} ms")
print(f"   FPS: {fps:.1f}")



all_preds = []
all_targets = []
times = []

# Open single log file for everything
os.makedirs('logs', exist_ok=True)
log_file = open('logs/pretrained_test.log', 'w')

# Write header

log_file.write("PRETRAINED MODEL TEST RESULTS (BEFORE FINE-TUNING)\n\n")

log_file.write("TEST PROGRESS:\n")
log_file.flush()

with torch.no_grad():
    for idx, img_path in enumerate(tqdm(test_images, file=sys.stdout)):
        try:
            # Log progress every 100 images
            if idx % 100 == 0:
                progress_msg = f"  Processed {idx}/{len(test_images)} images ({idx/len(test_images)*100:.1f}%)\n"
                log_file.write(progress_msg)
                log_file.flush()

            img = cv2.imread(str(img_path))
            if img is None:
                continue

            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            img = cv2.resize(img, (640, 640))
            img = img.astype('float32') / 255.0
            img = np.transpose(img, (2, 0, 1))
            img_tensor = torch.from_numpy(img).unsqueeze(0).to(device)

            if device.type == 'cuda':
                start = torch.cuda.Event(enable_timing=True)
                end = torch.cuda.Event(enable_timing=True)
                start.record()
                output = model(img_tensor)
                end.record()
                torch.cuda.synchronize()
                times.append(start.elapsed_time(end))

            preds = process_predictions(output, conf_thresh=0.25)

            label_file = labels_dir / f"{img_path.stem}.txt"
            targets = []
            if label_file.exists():
                with open(label_file, 'r') as f:
                    for line in f:
                        parts = line.strip().split()
                        if len(parts) >= 5:
                            cls, x, y, w, h = map(float, parts[:5])
                            targets.append([cls, x, y, w, h])

            all_preds.append(preds.cpu() if len(preds) > 0 else torch.tensor([]))
            all_targets.append(torch.tensor(targets) if targets else torch.tensor([]))

        except Exception as e:
            log_file.write(f"  Error on image {idx}: {str(e)}\n")
            all_preds.append(torch.tensor([]))
            all_targets.append(torch.tensor([]))

log_file.write(f"  Completed: {len(test_images)}/{len(test_images)} images (100.0%)\n\n")
log_file.flush()

print()  # New line after progress

log_file.write("ACCURACY METRICS:\n")
log_file.write(f"  mAP@0.5:0.95: {map_50_95:.3f} ({map_50_95*100:.1f}%)\n")
log_file.write(f"  mAP@0.5:      {map_50:.3f} ({map_50*100:.1f}%)\n")
log_file.write(f"  mAP@0.75:     {map_75:.3f} ({map_75*100:.1f}%)\n")
log_file.write(f"  Precision:    {prec_50:.3f} ({prec_50*100:.1f}%)\n")
log_file.write(f"  Recall:       {rec_50:.3f} ({rec_50*100:.1f}%)\n\n")

log_file.write("STATISTICS:\n")
log_file.write(f"  Images: {len(test_images)}\n")
log_file.write(f"  Predictions: {total_preds}\n")
log_file.write(f"  Ground truths: {total_tgts}\n")
log_file.write(f"  Avg preds/image: {total_preds/len(test_images):.1f}\n\n")

log_file.write("SPEED:\n")
log_file.write(f"  Inference: {avg_time:.2f} ms\n")
log_file.write(f"  FPS: {fps:.1f}\n\n")


log_file.close()

print("\n" + "="*70)
print("âš™ï¸  Creating Fine-tuning Configuration")
print("="*70)

# Use the standard YOLOv6n config as base
import shutil
import os

os.chdir("/content/YOLOv6")

# Copy the standard yolov6n config
shutil.copy("configs/yolov6n.py", "configs/yolov6n_finetune.py")

# Modify it for fine-tuning
with open("configs/yolov6n_finetune.py", 'r') as f:
    content = f.read()

# Update the learning rate for fine-tuning (lower than training from scratch)
if 'lr0' in content:
    content = content.replace('lr0=0.0032', 'lr0=0.01')  # Fine-tuning lr

# Update epochs
content = content.replace('epochs=400', f'epochs={EPOCHS}')

with open("configs/yolov6n_finetune.py", 'w') as f:
    f.write(content)

print("Configuration created: configs/yolov6n_finetune.py")
print("   Based on: configs/yolov6n.py")
print("   Learning rate: 0.01 (optimized for fine-tuning)")
print(f"   Epochs: {EPOCHS}")

import os
os.chdir("/content/YOLOv6")

# Show the problematic area
with open("yolov6/core/engine.py", 'r') as f:
    lines = f.readlines()
    for i in range(75, 95):  # Show lines 76-95
        print(f"{i+1:3d}: {lines[i]}", end='')

import os
os.chdir("/content/YOLOv6")

engine_file = "yolov6/core/engine.py"

with open(engine_file, 'r') as f:
    lines = f.readlines()

# Fix indentation - lines 81-87 need to be indented
for i in range(80, 87):  # Lines 81-87 (0-indexed: 80-86)
    if lines[i].startswith('        ') and not lines[i].startswith('            '):
        # Add 4 more spaces
        lines[i] = '    ' + lines[i]

with open(engine_file, 'w') as f:
    f.writelines(lines)

print(" Fixed indentation!")

# Verify
with open(engine_file, 'r') as f:
    lines = f.readlines()
    for i in range(78, 90):
        print(f"{i+1:3d}: {lines[i]}", end='')

import os
os.chdir("/content/YOLOv6")

!git checkout yolov6/core/engine.py 2>/dev/null

engine_file = "yolov6/core/engine.py"

# Read file
with open(engine_file, 'r') as f:
    content = f.read()

# Fix 1: Add weights_only=False
content = content.replace(
    "self.ckpt = torch.load(args.resume, map_location='cpu')",
    "self.ckpt = torch.load(args.resume, map_location='cpu', weights_only=False)"
)

# Fix 2: Replace strict loading block
content = content.replace(
    "            model.load_state_dict(resume_state_dict, strict=True)  # load",
    """            # Filter mismatched layers
            model_dict = model.state_dict()
            filtered = {k: v for k, v in resume_state_dict.items() if k in model_dict and v.shape == model_dict[k].shape}
            model.load_state_dict(filtered, strict=False)
            print(f'Loaded {len(filtered)}/{len(resume_state_dict)} layers')"""
)

# Write back
with open(engine_file, 'w') as f:
    f.write(content)

print(" Both fixes applied!")

# Show the fixed section
with open(engine_file, 'r') as f:
    lines = f.readlines()
    print("\nFixed section:")
    for i in range(45, 90):
        if i < len(lines):
            print(f"{i+1}: {lines[i]}", end='')

import os
os.chdir("/content/YOLOv6")

# Restore original
!git checkout yolov6/core/engine.py 2>&1 | grep -v "Updated"

engine_file = "yolov6/core/engine.py"

with open(engine_file, 'r') as f:
    content = f.read()

# Fix 1: PyTorch weights_only
content = content.replace(
    "self.ckpt = torch.load(args.resume, map_location='cpu')",
    "self.ckpt = torch.load(args.resume, map_location='cpu', weights_only=False)"
)

# Fix 2: Filtered layer loading
content = content.replace(
    "            model.load_state_dict(resume_state_dict, strict=True)  # load",
    """            # Filter mismatched layers
            model_dict = model.state_dict()
            filtered = {k: v for k, v in resume_state_dict.items() if k in model_dict and v.shape == model_dict[k].shape}
            model.load_state_dict(filtered, strict=False)"""
)

# Fix 3: Skip missing optimizer/scheduler
content = content.replace(
    "            self.start_epoch = self.ckpt['epoch'] + 1\n            self.optimizer.load_state_dict(self.ckpt['optimizer'])\n            self.scheduler.load_state_dict(self.ckpt['scheduler'])",
    """            self.start_epoch = self.ckpt.get('epoch', -1) + 1
            if 'optimizer' in self.ckpt and self.ckpt['optimizer']:
                self.optimizer.load_state_dict(self.ckpt['optimizer'])
            if 'scheduler' in self.ckpt and self.ckpt['scheduler']:
                self.scheduler.load_state_dict(self.ckpt['scheduler'])"""
)

# Fix 4: Skip missing EMA
content = content.replace(
    "            if self.main_process:\n                self.ema.ema.load_state_dict(self.ckpt['ema'].float().state_dict())\n                self.ema.updates = self.ckpt['updates']",
    """            if self.main_process and 'ema' in self.ckpt and self.ckpt['ema']:
                self.ema.ema.load_state_dict(self.ckpt['ema'].float().state_dict())
                self.ema.updates = self.ckpt.get('updates', 0)"""
)

# Fix 5: Skip missing results
content = content.replace(
    "self.evaluate_results = self.ckpt['results']",
    "self.evaluate_results = self.ckpt.get('results', (0, 0, 0))"
)

with open(engine_file, 'w') as f:
    f.write(content)

print(" All fixes applied! Training should start now.")

import os
os.chdir("/content/YOLOv6")

engine_file = "yolov6/core/engine.py"

with open(engine_file, 'r') as f:
    content = f.read()

# Fix missing 'results' key
content = content.replace(
    "self.evaluate_results = self.ckpt['results']",
    "self.evaluate_results = self.ckpt.get('results', (0, 0, 0))"
)

# Fix missing epoch attribute
content = content.replace(
    "strip_optimizer(save_ckpt_dir, self.epoch)",
    "strip_optimizer(save_ckpt_dir, getattr(self, 'epoch', self.max_epoch))"
)

with open(engine_file, 'w') as f:
    f.write(content)

print(" Fixed! Run Cell 11 again.")

pip uninstall tensorflow -y

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'


print(f"  TRAINING YOLOv6-n MODEL")

print(f"  Training for {EPOCHS} epochs")


# Train and capture full output
!python -u tools/train.py \
    --data-path /content/YOLOv6/data/pets.yaml \
    --conf-file configs/yolov6n_finetune.py \
    --batch-size {BATCH_SIZE} \
    --epochs {EPOCHS} \
    --device 0 \
    --name yolov6n_finetune \
    --workers 4 \
    --eval-final-only \
    2>&1 | tee logs/training_full.log

# Extract and format epoch summaries

print("TRAINING SUMMARY: \n")

import re

with open('logs/training_raw.log', 'r') as f:
    content = f.read()

# Find all epoch results using regex
pattern = r'(\d+)/49\s+([\d.]+)\s+([\d.]+)\s+(\d+)\s+([\d.]+)'
matches = re.findall(pattern, content)

# Write formatted log
with open('logs/training.log', 'w') as f:

    f.write("TRAINING LOG \n")

    f.write(f"Epochs: {EPOCHS}\n")
    f.write(f"Batch Size: {BATCH_SIZE}\n")

    f.write("EPOCH RESULTS:\n")
    f.write(f"{'Epoch':<10} {'LR':<12} {'IoU_Loss':<12} {'DFL_Loss':<12} {'Cls_Loss':<12}\n")


    # Print header to console
    print(f"{'Epoch':<10} {'LR':<12} {'IoU_Loss':<12} {'DFL_Loss':<12} {'Cls_Loss':<12}")


    # Get unique epochs (last occurrence of each)
    epoch_data = {}
    for match in matches:
        epoch, lr, iou, dfl, cls = match
        epoch_data[int(epoch)] = (lr, iou, dfl, cls)

    # Print and save all epochs
    for epoch in sorted(epoch_data.keys()):
        lr, iou, dfl, cls = epoch_data[epoch]
        line = f"{epoch :<10} {lr:<12} {iou:<12} {dfl:<12} {cls:<12}\n"
        f.write(line)
        print(line.strip())


print("\n TRAINING COMPLETE!")
print("   Formatted log: logs/training.log")
print("   Raw log: logs/training_raw.log")
print("   Checkpoint: runs/train/yolov6n_finetune/weights/best_ckpt.pt")

# Check training results
import os
os.chdir("/content/YOLOv6")

print("Checking training results...")

# Check if checkpoint exists
if os.path.exists("runs/train/yolov6n_finetune3/weights/best_ckpt.pt"):
    print(" Training checkpoint found!")
    checkpoint_path = "runs/train/yolov6n_finetune3/weights/best_ckpt.pt"
elif os.path.exists("runs/train/yolov6n_finetune2/weights/best_ckpt.pt"):
    print(" Training checkpoint found!")
    checkpoint_path = "runs/train/yolov6n_finetune2/weights/best_ckpt.pt"
elif os.path.exists("runs/train/yolov6n_finetune/weights/best_ckpt.pt"):
    print(" Training checkpoint found!")
    checkpoint_path = "runs/train/yolov6n_finetune/weights/best_ckpt.pt"
else:
    print(" No checkpoint found")
    checkpoint_path = None

# Parse training log
if os.path.exists('logs/training_full.log'):
    print("\n Extracting training results...")

    with open('logs/training_full.log', 'r') as f:
        content = f.read()

    # Find epoch results
    import re
    pattern = r'(\d+)/49\s+([\d.]+)\s+([\d.]+)\s+(\d+)\s+([\d.]+)'
    matches = re.findall(pattern, content)

    if matches:
        # Create formatted log
        with open('logs/training.log', 'w') as f:
            f.write("="*80 + "\n")
            f.write("TRAINING LOG\n")
            f.write("="*80 + "\n\n")
            f.write(f"{'Epoch':<10} {'LR':<12} {'IoU_Loss':<12} {'DFL_Loss':<12} {'Cls_Loss':<12}\n")
            f.write("-"*80 + "\n")

            epoch_data = {}
            for match in matches:
                epoch, lr, iou, dfl, cls = match
                epoch_data[int(epoch)] = (lr, iou, dfl, cls)

            for epoch in sorted(epoch_data.keys()):
                lr, iou, dfl, cls = epoch_data[epoch]
                line = f"{epoch}/49      {lr:<12} {iou:<12} {dfl:<12} {cls:<12}\n"
                f.write(line)

            f.write("="*80 + "\n")

        print(f" Processed {len(epoch_data)} epochs")
        print(" Training log saved to: logs/training.log")
    else:
        print("  Could not parse epoch data")
else:
    print(" training_full.log not found")

if checkpoint_path:
    print(f"\n YOU HAVE EVERYTHING YOU NEED!")
    print(f"   Trained model: {checkpoint_path}")
    print(f"   Training log: logs/training.log")

# Find the checkpoint
import os
from pathlib import Path

os.chdir("/content/YOLOv6")

print("ðŸ” Searching for checkpoints...")

# Search recursively
checkpoints = list(Path("runs/train").rglob("*.pt"))

if checkpoints:
    print(f"\n Found {len(checkpoints)} checkpoint(s):")
    for ckpt in checkpoints:
        size_mb = ckpt.stat().st_size / (1024*1024)
        print(f"   {ckpt} ({size_mb:.1f} MB)")

    # Use the best one
    best_ckpt = [c for c in checkpoints if 'best' in c.name]
    if best_ckpt:
        checkpoint_path = str(best_ckpt[0])
        print(f"\n Best checkpoint: {checkpoint_path}")
    else:
        checkpoint_path = str(checkpoints[-1])
        print(f"\n Using: {checkpoint_path}")

    # Copy to standard location for Cell 12
    os.makedirs("runs/train/yolov6n_finetune/weights", exist_ok=True)
    import shutil
    shutil.copy(checkpoint_path, "runs/train/yolov6n_finetune/weights/best_ckpt.pt")
    print(f" Copied to: runs/train/yolov6n_finetune/weights/best_ckpt.pt")
    print(f"\n Ready for Cell 12!")
else:
    print(" No checkpoints found!")
    print("   Check: runs/train/ directory")

import torch
import numpy as np

print(f"PyTorch: {torch.__version__}")
print(f"NumPy: {np.__version__}")

pip install "numpy<2.0"

import warnings
warnings.filterwarnings('ignore')

import os
import sys
import torch
import yaml
import numpy as np
import cv2
from pathlib import Path
from tqdm import tqdm


print("  TESTING TRAINED MODEL ON TARGET DATASET")


os.chdir("/content/YOLOv6")

# Load config
with open('/content/YOLOv6/data/pets.yaml', 'r') as f:
    data_config = yaml.safe_load(f)

print(f"\n  Dataset: {data_config['nc']} classes")

# Load TRAINED model
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print(f"  Loading trained model on {device}...")

ckpt = torch.load('runs/train/yolov6n_finetune/weights/best_ckpt.pt',
                  map_location=device, weights_only=False)
model = ckpt['model'].float().to(device).eval()

params = sum(p.numel() for p in model.parameters()) / 1e6
print(f"  Trained model loaded: {params:.2f}M parameters")

# Get test data
test_dir = Path("/content/YOLOv6/data/pets/images/val")
labels_dir = Path("/content/YOLOv6/data/pets/labels/val")
test_images = sorted(test_dir.glob('*.jpg'))
print(f"\n  Testing on {len(test_images)} images")

def process_predictions(output, conf_thresh=0.25, iou_thresh=0.45):
    """Process YOLOv6 output"""
    if isinstance(output, (list, tuple)):
        output = output[0]

    if len(output.shape) == 3:
        output = output[0]

    boxes = output[:, :4]
    objectness = output[:, 4]
    class_scores = output[:, 5:]

    class_conf, class_pred = class_scores.max(1)
    conf = objectness * class_conf

    mask = conf > conf_thresh

    if mask.sum() == 0:
        return torch.tensor([])

    boxes = boxes[mask]
    conf = conf[mask]
    class_pred = class_pred[mask]

    boxes = boxes / 640.0

    keep = []
    sorted_idx = conf.argsort(descending=True)

    while len(sorted_idx) > 0 and len(keep) < 100:
        i = sorted_idx[0]
        keep.append(i.item())

        if len(sorted_idx) == 1:
            break

        box = boxes[i]
        other = boxes[sorted_idx[1:]]

        b1_x1 = box[0] - box[2]/2
        b1_y1 = box[1] - box[3]/2
        b1_x2 = box[0] + box[2]/2
        b1_y2 = box[1] + box[3]/2

        b2_x1 = other[:, 0] - other[:, 2]/2
        b2_y1 = other[:, 1] - other[:, 3]/2
        b2_x2 = other[:, 0] + other[:, 2]/2
        b2_y2 = other[:, 1] + other[:, 3]/2

        inter_x1 = torch.maximum(b1_x1.clone(), b2_x1)
        inter_y1 = torch.maximum(b1_y1.clone(), b2_y1)
        inter_x2 = torch.minimum(b1_x2.clone(), b2_x2)
        inter_y2 = torch.minimum(b1_y2.clone(), b2_y2)

        inter = torch.clamp(inter_x2 - inter_x1, min=0) * torch.clamp(inter_y2 - inter_y1, min=0)
        area1 = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)
        area2 = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)
        union = area1 + area2 - inter

        iou = inter / (union + 1e-6)

        keep_mask = iou < iou_thresh
        sorted_idx = sorted_idx[1:][keep_mask]

    if len(keep) == 0:
        return torch.tensor([])

    return torch.cat([
        boxes[keep],
        conf[keep].unsqueeze(1),
        class_pred[keep].unsqueeze(1).float()
    ], dim=1)

def box_iou(box1, box2):
    """IoU for normalized boxes"""
    b1_x1, b1_y1 = box1[0] - box1[2]/2, box1[1] - box1[3]/2
    b1_x2, b1_y2 = box1[0] + box1[2]/2, box1[1] + box1[3]/2
    b2_x1, b2_y1 = box2[0] - box2[2]/2, box2[1] - box2[3]/2
    b2_x2, b2_y2 = box2[0] + box2[2]/2, box2[1] + box2[3]/2

    inter_x1 = max(b1_x1, b2_x1)
    inter_y1 = max(b1_y1, b2_y1)
    inter_x2 = min(b1_x2, b2_x2)
    inter_y2 = min(b1_y2, b2_y2)

    inter = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)
    area1 = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)
    area2 = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)
    union = area1 + area2 - inter

    return inter / union if union > 0 else 0

# Run evaluation
print("\n  Running inference...")

all_preds = []
all_targets = []
times = []

with torch.no_grad():
    for img_path in tqdm(test_images, desc="Testing"):
        try:
            img = cv2.imread(str(img_path))
            if img is None:
                continue

            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            img = cv2.resize(img, (640, 640))
            img = img.astype('float32') / 255.0
            img = np.transpose(img, (2, 0, 1))
            img_tensor = torch.from_numpy(img).unsqueeze(0).to(device)

            if device.type == 'cuda':
                start = torch.cuda.Event(enable_timing=True)
                end = torch.cuda.Event(enable_timing=True)
                start.record()
                output = model(img_tensor)
                end.record()
                torch.cuda.synchronize()
                times.append(start.elapsed_time(end))

            preds = process_predictions(output, conf_thresh=0.25)

            label_file = labels_dir / f"{img_path.stem}.txt"
            targets = []
            if label_file.exists():
                with open(label_file, 'r') as f:
                    for line in f:
                        parts = line.strip().split()
                        if len(parts) >= 5:
                            cls, x, y, w, h = map(float, parts[:5])
                            targets.append([cls, x, y, w, h])

            all_preds.append(preds.cpu() if len(preds) > 0 else torch.tensor([]))
            all_targets.append(torch.tensor(targets) if targets else torch.tensor([]))

        except:
            all_preds.append(torch.tensor([]))
            all_targets.append(torch.tensor([]))

print()

# Calculate mAP
def calculate_ap(predictions, targets, iou_thresh=0.5):
    """Calculate AP"""
    tp_list = []
    conf_list = []
    total_gt = sum(len(t) for t in targets)

    if total_gt == 0:
        return 0.0, 0.0, 0.0

    for pred, tgt in zip(predictions, targets):
        if len(pred) == 0:
            continue

        matched = set()
        for i in range(len(pred)):
            pred_box = pred[i, :4].numpy()
            pred_conf = pred[i, 4].item()

            best_iou = 0
            best_j = -1

            if len(tgt) > 0:
                for j in range(len(tgt)):
                    if j in matched:
                        continue
                    tgt_box = tgt[j, 1:5].numpy()
                    iou = box_iou(pred_box, tgt_box)
                    if iou > best_iou:
                        best_iou = iou
                        best_j = j

            if best_iou >= iou_thresh and best_j != -1:
                tp_list.append(1)
                matched.add(best_j)
            else:
                tp_list.append(0)

            conf_list.append(pred_conf)

    if len(tp_list) == 0:
        return 0.0, 0.0, 0.0

    indices = np.argsort(conf_list)[::-1]
    tp = np.array(tp_list)[indices]

    tp_cumsum = np.cumsum(tp)
    fp_cumsum = np.cumsum(1 - tp)

    recalls = tp_cumsum / total_gt
    precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-10)

    ap = 0
    for i in range(len(recalls) - 1):
        ap += (recalls[i+1] - recalls[i]) * precisions[i+1]

    return ap, precisions[-1], recalls[-1]

# Calculate metrics
map_50, prec_50, rec_50 = calculate_ap(all_preds, all_targets, 0.5)
map_75, _, _ = calculate_ap(all_preds, all_targets, 0.75)

aps = [calculate_ap(all_preds, all_targets, iou)[0] for iou in np.arange(0.5, 1.0, 0.05)]
map_50_95 = np.mean(aps)

total_preds = sum(len(p) for p in all_preds)
total_tgts = sum(len(t) for t in all_targets)
avg_time = np.mean(times)
fps = 1000.0 / avg_time

# Load pretrained results for comparison
try:
    with open('logs/pretrained_test.log', 'r') as f:
        content = f.read()
        import re
        match = re.search(r'mAP@0.5:0.95:\s*([\d.]+)', content)
        map_50_95_pre = float(match.group(1)) if match else 0.022
except:
    map_50_95_pre = 0.022

improvement = ((map_50_95 - map_50_95_pre) / map_50_95_pre * 100) if map_50_95_pre > 0 else 0


print("  TRAINED MODEL TEST RESULTS (AFTER TRAINING)")

print(f"\n  Accuracy Metrics:")
print(f"   mAP@0.5:0.95: {map_50_95:.3f}  ({map_50_95*100:.1f}%)")
print(f"   mAP@0.5:      {map_50:.3f}  ({map_50*100:.1f}%)")
print(f"   mAP@0.75:     {map_75:.3f}  ({map_75*100:.1f}%)")
print(f"   Precision:    {prec_50:.3f}  ({prec_50*100:.1f}%)")
print(f"   Recall:       {rec_50:.3f}  ({rec_50*100:.1f}%)")

print(f"\n  Statistics:")
print(f"   Images: {len(test_images)}")
print(f"   Predictions: {total_preds}")
print(f"   Ground truths: {total_tgts}")
print(f"   Avg preds/image: {total_preds/len(test_images):.1f}")

print(f"\nâš¡ Speed:")
print(f"   Inference: {avg_time:.2f} ms")
print(f"   FPS: {fps:.1f}")

print(f"\n  IMPROVEMENT vs Pretrained:")
print(f"   Before: mAP@0.5:0.95 = {map_50_95_pre:.3f}")
print(f"   After:  mAP@0.5:0.95 = {map_50_95:.3f}")
print(f"   Gain: +{improvement:.0f}%")



# Save log
log_file = open('logs/finetuned_test.log', 'w')

log_file.write("TRAINED MODEL TEST RESULTS (AFTER TRAINING)\n")


log_file.write("ACCURACY METRICS:\n")
log_file.write(f"  mAP@0.5:0.95: {map_50_95:.3f} ({map_50_95*100:.1f}%)\n")
log_file.write(f"  mAP@0.5:      {map_50:.3f} ({map_50*100:.1f}%)\n")
log_file.write(f"  mAP@0.75:     {map_75:.3f} ({map_75*100:.1f}%)\n")
log_file.write(f"  Precision:    {prec_50:.3f} ({prec_50*100:.1f}%)\n")
log_file.write(f"  Recall:       {rec_50:.3f} ({rec_50*100:.1f}%)\n\n")

log_file.write("STATISTICS:\n")
log_file.write(f"  Images: {len(test_images)}\n")
log_file.write(f"  Predictions: {total_preds}\n")
log_file.write(f"  Ground truths: {total_tgts}\n")
log_file.write(f"  Avg preds/image: {total_preds/len(test_images):.1f}\n\n")

log_file.write("SPEED:\n")
log_file.write(f"  Inference: {avg_time:.2f} ms\n")
log_file.write(f"  FPS: {fps:.1f}\n\n")

log_file.write("COMPARISON:\n")
log_file.write(f"  Before (pretrained): {map_50_95_pre:.3f}\n")
log_file.write(f"  After (trained):     {map_50_95:.3f}\n")
log_file.write(f"  Improvement:         +{improvement:.0f}%\n\n")


log_file.close()

print("\n  Complete! Log saved to: logs/finetuned_test.log")

import warnings
warnings.filterwarnings('ignore')

import os
import sys
import torch
import yaml
import numpy as np
import cv2
from pathlib import Path
from tqdm import tqdm
import time

# ============================================================================
# SETUP LOG FILE
# ============================================================================
os.makedirs('logs', exist_ok=True)
log_file = open('logs/finetuned_test.log', 'w')

def log_print(msg):
    """Print to both console and file"""
    print(msg)
    log_file.write(msg + '\n')
    log_file.flush()

# ============================================================================
# START TESTING
# ============================================================================

log_print("  TESTING TRAINED MODEL ON TARGET DATASET")

os.chdir("/content/YOLOv6")

# Load config
with open('/content/YOLOv6/data/pets.yaml', 'r') as f:
    data_config = yaml.safe_load(f)

log_print(f"\n  Dataset: {data_config['nc']} classes")

# Load TRAINED model
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
log_print(f"  Loading trained model on {device}...")

ckpt = torch.load('runs/train/yolov6n_finetune/weights/best_ckpt.pt',
                  map_location=device, weights_only=False)
model = ckpt['model'].float().to(device).eval()

params = sum(p.numel() for p in model.parameters()) / 1e6
log_print(f"  Trained model loaded: {params:.2f}M parameters")

# Get test data
test_dir = Path("/content/YOLOv6/data/pets/images/val")
labels_dir = Path("/content/YOLOv6/data/pets/labels/val")
test_images = sorted(test_dir.glob('*.jpg'))
log_print(f"\n   Testing on {len(test_images)} images")

def process_predictions(output, conf_thresh=0.25, iou_thresh=0.45):
    """Process YOLOv6 output"""
    if isinstance(output, (list, tuple)):
        output = output[0]

    if len(output.shape) == 3:
        output = output[0]

    boxes = output[:, :4]
    objectness = output[:, 4]
    class_scores = output[:, 5:]

    class_conf, class_pred = class_scores.max(1)
    conf = objectness * class_conf

    mask = conf > conf_thresh

    if mask.sum() == 0:
        return torch.tensor([])

    boxes = boxes[mask]
    conf = conf[mask]
    class_pred = class_pred[mask]

    boxes = boxes / 640.0

    keep = []
    sorted_idx = conf.argsort(descending=True)

    while len(sorted_idx) > 0 and len(keep) < 100:
        i = sorted_idx[0]
        keep.append(i.item())

        if len(sorted_idx) == 1:
            break

        box = boxes[i]
        other = boxes[sorted_idx[1:]]

        b1_x1 = box[0] - box[2]/2
        b1_y1 = box[1] - box[3]/2
        b1_x2 = box[0] + box[2]/2
        b1_y2 = box[1] + box[3]/2

        b2_x1 = other[:, 0] - other[:, 2]/2
        b2_y1 = other[:, 1] - other[:, 3]/2
        b2_x2 = other[:, 0] + other[:, 2]/2
        b2_y2 = other[:, 1] + other[:, 3]/2

        inter_x1 = torch.maximum(b1_x1.clone(), b2_x1)
        inter_y1 = torch.maximum(b1_y1.clone(), b2_y1)
        inter_x2 = torch.minimum(b1_x2.clone(), b2_x2)
        inter_y2 = torch.minimum(b1_y2.clone(), b2_y2)

        inter = torch.clamp(inter_x2 - inter_x1, min=0) * torch.clamp(inter_y2 - inter_y1, min=0)
        area1 = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)
        area2 = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)
        union = area1 + area2 - inter

        iou = inter / (union + 1e-6)

        keep_mask = iou < iou_thresh
        sorted_idx = sorted_idx[1:][keep_mask]

    if len(keep) == 0:
        return torch.tensor([])

    return torch.cat([
        boxes[keep],
        conf[keep].unsqueeze(1),
        class_pred[keep].unsqueeze(1).float()
    ], dim=1)

def box_iou(box1, box2):
    """IoU for normalized boxes"""
    b1_x1, b1_y1 = box1[0] - box1[2]/2, box1[1] - box1[3]/2
    b1_x2, b1_y2 = box1[0] + box1[2]/2, box1[1] + box1[3]/2
    b2_x1, b2_y1 = box2[0] - box2[2]/2, box2[1] - box2[3]/2
    b2_x2, b2_y2 = box2[0] + box2[2]/2, box2[1] + box2[3]/2

    inter_x1 = max(b1_x1, b2_x1)
    inter_y1 = max(b1_y1, b2_y1)
    inter_x2 = min(b1_x2, b2_x2)
    inter_y2 = min(b1_y2, b2_y2)

    inter = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)
    area1 = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)
    area2 = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)
    union = area1 + area2 - inter

    return inter / union if union > 0 else 0

# Run evaluation
log_print("\n  Running inference...")
start_time = time.time()

all_preds = []
all_targets = []
times = []

total = len(test_images)
log_interval = max(1, total // 10)  # Log every 10%

with torch.no_grad():
    pbar = tqdm(test_images, desc="Testing")
    for idx, img_path in enumerate(pbar):
        try:
            img = cv2.imread(str(img_path))
            if img is None:
                continue

            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            img = cv2.resize(img, (640, 640))
            img = img.astype('float32') / 255.0
            img = np.transpose(img, (2, 0, 1))
            img_tensor = torch.from_numpy(img).unsqueeze(0).to(device)

            if device.type == 'cuda':
                start = torch.cuda.Event(enable_timing=True)
                end = torch.cuda.Event(enable_timing=True)
                start.record()
                output = model(img_tensor)
                end.record()
                torch.cuda.synchronize()
                times.append(start.elapsed_time(end))

            preds = process_predictions(output, conf_thresh=0.25)

            label_file = labels_dir / f"{img_path.stem}.txt"
            targets = []
            if label_file.exists():
                with open(label_file, 'r') as f:
                    for line in f:
                        parts = line.strip().split()
                        if len(parts) >= 5:
                            cls, x, y, w, h = map(float, parts[:5])
                            targets.append([cls, x, y, w, h])

            all_preds.append(preds.cpu() if len(preds) > 0 else torch.tensor([]))
            all_targets.append(torch.tensor(targets) if targets else torch.tensor([]))

            # Log progress to file
            if (idx + 1) % log_interval == 0 or (idx + 1) == total:
                progress = (idx + 1) / total * 100
                elapsed = time.time() - start_time
                rate = (idx + 1) / elapsed
                msg = f"   Testing: {idx+1}/{total} ({progress:.0f}%) [{elapsed:.1f}s, {rate:.2f}it/s]"
                log_file.write(msg + '\n')
                log_file.flush()

        except Exception as e:
            all_preds.append(torch.tensor([]))
            all_targets.append(torch.tensor([]))

total_time = time.time() - start_time
avg_rate = total / total_time

# Log final progress summary
log_file.write(f"   Testing: 100% | {total}/{total} [{total_time:.2f}s, {avg_rate:.2f}it/s]\n")
log_file.flush()

log_print(f"  Inference completed in {total_time:.2f}s ({avg_rate:.2f} it/s)")

# Calculate mAP
def calculate_ap(predictions, targets, iou_thresh=0.5):
    """Calculate AP"""
    tp_list = []
    conf_list = []
    total_gt = sum(len(t) for t in targets)

    if total_gt == 0:
        return 0.0, 0.0, 0.0

    for pred, tgt in zip(predictions, targets):
        if len(pred) == 0:
            continue

        matched = set()
        for i in range(len(pred)):
            pred_box = pred[i, :4].numpy()
            pred_conf = pred[i, 4].item()

            best_iou = 0
            best_j = -1

            if len(tgt) > 0:
                for j in range(len(tgt)):
                    if j in matched:
                        continue
                    tgt_box = tgt[j, 1:5].numpy()
                    iou = box_iou(pred_box, tgt_box)
                    if iou > best_iou:
                        best_iou = iou
                        best_j = j

            if best_iou >= iou_thresh and best_j != -1:
                tp_list.append(1)
                matched.add(best_j)
            else:
                tp_list.append(0)

            conf_list.append(pred_conf)

    if len(tp_list) == 0:
        return 0.0, 0.0, 0.0

    indices = np.argsort(conf_list)[::-1]
    tp = np.array(tp_list)[indices]

    tp_cumsum = np.cumsum(tp)
    fp_cumsum = np.cumsum(1 - tp)

    recalls = tp_cumsum / total_gt
    precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-10)

    ap = 0
    for i in range(len(recalls) - 1):
        ap += (recalls[i+1] - recalls[i]) * precisions[i+1]

    return ap, precisions[-1], recalls[-1]

log_print("\n  Calculating metrics...")

# Calculate metrics
map_50, prec_50, rec_50 = calculate_ap(all_preds, all_targets, 0.5)
map_75, _, _ = calculate_ap(all_preds, all_targets, 0.75)

aps = [calculate_ap(all_preds, all_targets, iou)[0] for iou in np.arange(0.5, 1.0, 0.05)]
map_50_95 = np.mean(aps)

total_preds = sum(len(p) for p in all_preds)
total_tgts = sum(len(t) for t in all_targets)
avg_time = np.mean(times) if times else 0
fps = 1000.0 / avg_time if avg_time > 0 else 0

# Load pretrained results for comparison
try:
    with open('logs/pretrained_test.log', 'r') as f:
        content = f.read()
        import re
        match = re.search(r'mAP@0.5:0.95:\s*([\d.]+)', content)
        map_50_95_pre = float(match.group(1)) if match else 0.022
except:
    map_50_95_pre = 0.022

improvement = ((map_50_95 - map_50_95_pre) / map_50_95_pre * 100) if map_50_95_pre > 0 else 0

# ============================================================================
# DISPLAY AND LOG RESULTS
# ============================================================================

log_print("  TRAINED MODEL TEST RESULTS (AFTER TRAINING)")


log_print(f"\n  Accuracy Metrics:")
log_print(f"   mAP@0.5:0.95: {map_50_95:.3f}  ({map_50_95*100:.1f}%)")
log_print(f"   mAP@0.5:      {map_50:.3f}  ({map_50*100:.1f}%)")
log_print(f"   mAP@0.75:     {map_75:.3f}  ({map_75*100:.1f}%)")
log_print(f"   Precision:    {prec_50:.3f}  ({prec_50*100:.1f}%)")
log_print(f"   Recall:       {rec_50:.3f}  ({rec_50*100:.1f}%)")

log_print(f"\n  Statistics:")
log_print(f"   Images: {len(test_images)}")
log_print(f"   Predictions: {total_preds}")
log_print(f"   Ground truths: {total_tgts}")
log_print(f"   Avg preds/image: {total_preds/len(test_images):.1f}")

log_print(f"\n  Speed:")
log_print(f"   Inference: {avg_time:.2f} ms")
log_print(f"   FPS: {fps:.1f}")
log_print(f"   Total time: {total_time:.2f}s")

log_print(f"\n  IMPROVEMENT vs Pretrained:")
log_print(f"   Before: mAP@0.5:0.95 = {map_50_95_pre:.3f}")
log_print(f"   After:  mAP@0.5:0.95 = {map_50_95:.3f}")
log_print(f"   Gain: +{improvement:.0f}%")


log_print("  Complete! Log saved to: logs/finetuned_test.log")

# Close log file
log_file.close()

from google.colab import files
files.download("runs/train/yolov6n_finetune/weights/last_ckpt.pt")